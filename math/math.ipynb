{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f208f7",
   "metadata": {},
   "source": [
    "\n",
    "## Theory and references\n",
    "\n",
    "#### Cross Correlation\n",
    "\n",
    "of two signals is defined: \n",
    " $[f*g](t) = \\sum_{i=1}^{n} f(t) g(t-\\tau)$\n",
    "\n",
    "the characteristic time $\\tau_{lag}$ can be computed: \n",
    "\n",
    " $\\tau_{lag} = argmax|[f*g](t)|$\n",
    "\n",
    "#### Entropy (Shannon, 1948): \n",
    "\n",
    "Given a discrete random variable $X$, with possible outcomes $ x_{1},...,x_{n} $ which occur with probability $  \\mathrm {P} (x_{1}),...,\\mathrm {P} (x_{n}) $ the entropy (units of nats) of $X$ is defined as: <br>\n",
    "\n",
    "$ H(X) =  - \\sum_{i=1}^{n} P(x) \\ln P(x) $\n",
    "\n",
    "#### Joint Entropy:\n",
    "of two discrete random variables $X$ and $Y$ is defined as the entropy of the joint distribution of $X$ and $Y$:\n",
    "\n",
    "$ H(X,Y) =  - \\sum_{i=1}^{n} P(x,y) \\ln P(x,y) $\n",
    "\n",
    "\n",
    "#### Conditional Entropy: \n",
    "\n",
    "The amount of information needed to describe the outcome of a random variable $Y$ given that the value of another random variable $X$ is known. Here, information is measured in shannons, nats, or hartleys. The entropy of $Y$ conditioned on $X$ is:\n",
    "\n",
    "$ H (Y|X) = -\\sum p(x,y)\\ln {\\frac {p(x,y)}{p(x)}} $\n",
    "\n",
    "\n",
    "#### Relative Entropy, aka K-L Divergence,\n",
    "\n",
    "The Relative Entropy (aka K-L divergence, $ D_{\\text{KL}}(P\\parallel Q)$ ), which measures how one probability distribution $P(x)$ is different from a second $Q(x)$ is defined as:\n",
    "\n",
    "$ D_{\\text{KL}}(P\\parallel Q)=\\sum _{x\\in {\\mathcal {X}}}P(x)\\ln \\left({\\frac {P(x)}{Q(x)}}\\right) $\n",
    "\n",
    "#### Jensen Shannon Distance:\n",
    "\n",
    "The Jensen Shannon Distance (JSD) also measures how one probability distribution $P(x)$ is different from a second $Q(x)$, but has desirable properties of always being finite and symmetric: \n",
    "\n",
    "$ JSD(X) = \\sqrt{\\frac{D(p \\parallel m) + D(q \\parallel m)}{2}} $\n",
    "\n",
    "where $D(x \\parallel y)$ is the K-L Divergence, defined above.\n",
    "\n",
    "\n",
    "#### Mutual information\n",
    "\n",
    "measures how much information can be obtained about one random variable by observing another. The mutual information of $X$ relative to $Y$ (which represents conceptually the average amount of information about $X$ that can be gained by observing $Y$ is given by:\n",
    "\n",
    "$ I(X; Y)=H(X)− H(X|Y)= -\\sum p(x,y)\\ln \\frac{p(x,y)}{p(x) p(y)} $\n",
    "\n",
    "#### Transfer entropy (Schreiber, 2000)\n",
    "\n",
    "is the amount of directed (time-asymmetric) transfer of information between two random processes. Transfer entropy from a process X to another process Y is the amount of uncertainty reduced in future values of Y by knowing the past values of X given past values of Y.\n",
    "\n",
    "$ T_{X→Y} = \\sum p(y_{t+1}, y_{t}, x_{t}) ln( \\frac{p(y_{t+1} | y_{t} , x_{t})} {p(y_{t+1} | y_{t})}) $\n",
    "\n",
    "Can be thought of as the deviation from independence\n",
    "(in bits) of the state transition (from the previous state\n",
    "to the next state) of an information destination X from\n",
    "the (previous) state of an information source Y\n",
    "\n",
    "Transfer entropy can be thought of as Conditional mutual Information (Lizier, 2008): \n",
    "\n",
    "$ T_{X→Y} = I(X ; Y{t+1}|Y) = H(Y_{t+1}|Y) − H(Y_{t+1}|Y,X) $ \n",
    "\n",
    "#### References\n",
    "\n",
    "Shannon, C. E. (1948). A mathematical theory of communication. The Bell system technical journal, 27(3), 379-423.\n",
    "\n",
    "Schreiber, T. (2000). Measuring information transfer. Physical review letters, 85(2), 461.\n",
    "\n",
    "Lizier, J. T., Prokopenko, M., & Zomaya, A. Y. (2008). Local information transfer as a spatiotemporal filter for complex systems. Physical Review E, 77(2), 026110.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
